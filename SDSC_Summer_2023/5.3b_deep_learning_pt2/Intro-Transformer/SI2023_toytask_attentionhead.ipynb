{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1vSsR_1UeJv"
   },
   "source": [
    "# We make up a toy problem to gain intuition about attenion heads\n",
    "\n",
    "Let's use the following list of words (and a special token):\n",
    "<Start>, the, man, chicken, ordered,woman, beef    7 words,so V=7, and token ids are 1 to 7\n",
    "\n",
    "This our corpus:  \n",
    "<Start> man ordered the chicken\n",
    "\n",
    "<Start> woman ordered the beef\n",
    "\n",
    "The corpus will be translated to 2 sequences of token ids as input.\n",
    "\n",
    "Exercise Instructions:\n",
    "Let's examine the Attention Wt matrix to see how dependencies might be encoded  (look for '<<<-----' markers)\n",
    "\n",
    "your item 1:\n",
    "Run the notebook, do print screen for output predictions and attention weights for 1 case that makes good predictions.  Write 1 or 2 sentences on how attention weights are coding the dependency\n",
    "\n",
    "your item 2:\n",
    "Try different values of H parameter (20 works well I think, but not always).\n",
    "\n",
    "Try 10, 20, 40. Try at least a few runs,\n",
    "  Write 1 or 2 sentences summarizing if H larger/smaller than 20 is noticably better or worse (look at the predictions for token 'the' to see if it predicts 'chicken' or 'beef' appropriately, and look at the loss value from training. Note, the loss is taken over all predictions, not just 'chicken' or 'beef')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DIvdHFC-UeJz"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Let's first make a VxV matrix that represents 'association' between row and col\n",
    "#eg <start>    predicts 'man' with value 1,\n",
    "#      'the'   predicts 'beef  or chicken' with value 0.5,0.5 \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#tf.keras.utils.set_random_seed(777)\n",
    "\n",
    "#set the embedding/attention head size parameters\n",
    "E = 7  #size of embedding layer\n",
    "H = 20  #size of attention head    #<<<<<<<<<---- try 10,20,40 ---\n",
    "                                     # How does H parameter affect the Attent. Wts?  hint: look at model summary output\n",
    "\n",
    "#Note B will be the batch size, it will be set to number of sequences we create\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hwK-P0HpUeJ1"
   },
   "outputs": [],
   "source": [
    "# Set up sequence input\n",
    "if 1:\n",
    "    colnames  =  [\"<ST>\", \"the\", \"man\",\"chkn\",\"ordrd\",\"woman\",\"beef\"]\n",
    "    V    = len(colnames)\n",
    "    #------ make a sequence of length T --------------\n",
    "    sequence2use = np.asarray([[1,3,5,2,4],[1,6,5,2,7]])  #start is first token, with array index 0, \n",
    "    B,T = sequence2use.shape\n",
    "    sequence2pred       = np.zeros((B,T),dtype=int)\n",
    "    for bi in range(B):\n",
    "      sequence2pred[bi,0:-1] = sequence2use[bi,1:]\n",
    "      sequence2pred[bi,-1]   = sequence2use[bi,0]\n",
    "\n",
    "#set up a dataframe info for nice printouts later\n",
    "rownamesxb =list(list())\n",
    "for bi in range(B):\n",
    "    rownames=list()\n",
    "    for i in range(T):\n",
    "        rownames.append(str(i)+' '+colnames[sequence2use[bi,i]-1])\n",
    "    rownamesxb.append(rownames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MP-smInjguz8",
    "outputId": "7e8b3151-821b-48af-a15d-8f41b1689ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 5 2 4]\n",
      " [1 6 5 2 7]]\n",
      "[[3 5 2 4 1]\n",
      " [6 5 2 7 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['0 <ST>', '1 man', '2 ordrd', '3 the', '4 chkn'],\n",
       " ['0 <ST>', '1 woman', '2 ordrd', '3 the', '4 beef']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sequence2use)\n",
    "print(sequence2pred)\n",
    "rownamesxb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3X8k7jSsvNY6",
    "outputId": "c4c15e8e-add7-4c48-8c43-6994b3c0ed9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 1)\n",
      "(2, 5, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 00:13:15.202947: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-06-29 00:13:15.202984: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (exp-10-36): /proc/driver/nvidia/version does not exist\n",
      "2023-06-29 00:13:15.203753: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Now set up training data  Batch size is just 1  \n",
    "#set up token sequences of id numbers  \n",
    "Xtrain  = np.zeros((B,T,1)) #sequence2use.copy()\n",
    "for bi in range(B):\n",
    "    for ti in range(T):\n",
    "       Xtrain[bi,ti]=sequence2use[bi,ti]-1  #Xtrain_ids[bi,ti]-1  #index starts at 0 so subtract1\n",
    "\n",
    "\n",
    "#make postrain same batch size as X,Y\n",
    "Postrain=np.zeros((B,T,1))\n",
    "for bi in range(B):\n",
    "  Postrain[bi,:,0] = np.arange(T)  #set Position to integer 1...T\n",
    "#Postrain  = tf.expand_dims(Postrain,axis=2)     #add batch dim to input\n",
    "print(Postrain.shape)\n",
    "\n",
    "Ytrain=sequence2pred.copy()\n",
    "for bi in range(B):\n",
    "   for ti in range(T):\n",
    "      Ytrain[bi,ti]=Ytrain[bi,ti]-1  #index starts at 0 so subtract1\n",
    "Ytrain = tf.expand_dims(Ytrain,axis=2)\n",
    "print(Ytrain.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NLj9qugRf3-0"
   },
   "outputs": [],
   "source": [
    "#Now set up model related values\n",
    "scale_value    = np.divide(1,np.sqrt(H)) #use H b/c it's dimension of Qmat, Kmat\n",
    "scale_constant =tf.constant(scale_value,dtype=tf.float32)  #this will scale Attn Wts before softmax\n",
    "\n",
    "#make TxT matrix of scale to apply elememnt wise  \n",
    "initializer = tf.keras.initializers.Constant(scale_value)\n",
    "scale_matrix = initializer(shape=(T,T))\n",
    "\n",
    "#Make a mask\n",
    "Msk=np.ones((T,T))\n",
    "Msk = tf.convert_to_tensor(np.tril(Msk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4enEgTnBtm4",
    "outputId": "b6bcedee-49bb-491f-a41d-037f0ab6b0c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [3.]]\n",
      "\n",
      " [[0.]\n",
      "  [5.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [6.]]]\n"
     ]
    }
   ],
   "source": [
    "#show the Xtrain input\n",
    "print(Xtrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RhVqmPEaUeJ2"
   },
   "outputs": [],
   "source": [
    "#Now build model to learn transformation for Q,K,V matrices\n",
    "\n",
    "Xseqids_in       = tf.keras.layers.Input(shape=(T),name='Xseqids')        #this takes in the token ids and makes TxV input\n",
    "Xembed         = tf.keras.layers.Embedding(V,V,input_length=T,name='XEmbed')(Xseqids_in) #1st V is numbr of possible ids, 2nd V is embedding dimension (columns), T is rows\n",
    "\n",
    "#Xsequence     = tf.keras.layers.Input(shape=(T,V)) #the batch size is left unspecified\n",
    "Pos_Input     = tf.keras.layers.Input(shape=(T),name='PosInput') #just the t=1...T integer\n",
    "Pos_Embed     = tf.keras.layers.Embedding(T,V, input_length=T,name='PosEmbed')(Pos_Input) #input will be a number between 1 and T for length=T times\n",
    "Xinputs       = tf.keras.layers.Add(name='XwithPos')([Xembed, Pos_Embed])\n",
    "\n",
    "#now feed to Q,K,V transformations\n",
    "Qmat       = tf.keras.layers.Dense(H,activation='linear',use_bias=False,name='Qmat')(Xinputs) #so for BxTxV input this should be T*V to H dense?\n",
    "Kmat       = tf.keras.layers.Dense(H,activation='linear',use_bias=False,name='Kmat')(Xinputs) \n",
    "Vmat       = tf.keras.layers.Dense(V,activation='linear',use_bias=False,name='Vmat')(Xinputs) #Vmat output is V columns to use it as output\n",
    "\n",
    "#now apply QtoK take softmax, scale it , apply to V\n",
    "QK          = tf.keras.layers.Dot(axes=(2))([Qmat,Kmat])  #it will treat each Batch item separately\n",
    "QKscaled    = tf.keras.layers.Lambda(lambda x: x * scale_constant)(QK)    #for each x in QK mult by scale\n",
    "Attn_Wts     = tf.keras.layers.Softmax(axis=2,name='AttnWts')(QKscaled, mask=Msk)        #apply mask, apply softmax across row elemnts\n",
    "\n",
    "Vout       = tf.keras.layers.Dot(axes=(2,1),name='Voutput')([Attn_Wts,Vmat])     #turn this off for our tests, and ..\n",
    "\n",
    "#add this Add layer, with Xinputs skipped ahead to this input\n",
    "VoutandXinput = tf.keras.layers.Add(name='skipadd')([Vout,Xinputs])  #<<< this is a skip connection with plus (aka residual skip connection)\n",
    "\n",
    "Pout  = tf.keras.layers.Activation(activation='sigmoid')(VoutandXinput)\n",
    "\n",
    "my_attn_model   = tf.keras.Model(inputs = [Xseqids_in,Pos_Input], outputs=Pout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4kx79ECm-2E",
    "outputId": "079028c9-55a5-4694-8e65-b8c4bc76a743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Xseqids (InputLayer)           [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " PosInput (InputLayer)          [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " XEmbed (Embedding)             (None, 5, 7)         49          ['Xseqids[0][0]']                \n",
      "                                                                                                  \n",
      " PosEmbed (Embedding)           (None, 5, 7)         35          ['PosInput[0][0]']               \n",
      "                                                                                                  \n",
      " XwithPos (Add)                 (None, 5, 7)         0           ['XEmbed[0][0]',                 \n",
      "                                                                  'PosEmbed[0][0]']               \n",
      "                                                                                                  \n",
      " Qmat (Dense)                   (None, 5, 20)        140         ['XwithPos[0][0]']               \n",
      "                                                                                                  \n",
      " Kmat (Dense)                   (None, 5, 20)        140         ['XwithPos[0][0]']               \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 5, 5)         0           ['Qmat[0][0]',                   \n",
      "                                                                  'Kmat[0][0]']                   \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 5, 5)         0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " AttnWts (Softmax)              (None, 5, 5)         0           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " Vmat (Dense)                   (None, 5, 7)         49          ['XwithPos[0][0]']               \n",
      "                                                                                                  \n",
      " Voutput (Dot)                  (None, 5, 7)         0           ['AttnWts[0][0]',                \n",
      "                                                                  'Vmat[0][0]']                   \n",
      "                                                                                                  \n",
      " skipadd (Add)                  (None, 5, 7)         0           ['Voutput[0][0]',                \n",
      "                                                                  'XwithPos[0][0]']               \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 5, 7)         0           ['skipadd[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 413\n",
      "Trainable params: 413\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#configur loss and optimizer  \n",
    "#Note: Sparse loss means Keras expect target to be an integer indicating which output should be 'on'\n",
    "#    For the matrix output, Keras assumes to consider output predictions in each row\n",
    "#my_attn_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01), \n",
    "my_attn_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), \n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)) #logits=T means it is a linear output -inf to +inf\n",
    "\n",
    "my_attn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q52nfxcvYAL_",
    "outputId": "7015c7c2-7d5d-4b4c-a798-c4d414bc9b2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 - 0s - loss: 1.9300 - 391ms/epoch - 391ms/step\n",
      "Epoch 2/10\n",
      "1/1 - 0s - loss: 1.9267 - 1ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "1/1 - 0s - loss: 1.9234 - 1ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "1/1 - 0s - loss: 1.9202 - 946us/epoch - 946us/step\n",
      "Epoch 5/10\n",
      "1/1 - 0s - loss: 1.9169 - 1ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "1/1 - 0s - loss: 1.9137 - 1ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "1/1 - 0s - loss: 1.9104 - 947us/epoch - 947us/step\n",
      "Epoch 8/10\n",
      "1/1 - 0s - loss: 1.9072 - 940us/epoch - 940us/step\n",
      "Epoch 9/10\n",
      "1/1 - 0s - loss: 1.9040 - 929us/epoch - 929us/step\n",
      "Epoch 10/10\n",
      "1/1 - 0s - loss: 1.9008 - 983us/epoch - 983us/step\n",
      "Epoch 1/10\n",
      "1/1 - 0s - loss: 0.1658 - 3ms/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "1/1 - 0s - loss: 0.1657 - 1ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "1/1 - 0s - loss: 0.1657 - 982us/epoch - 982us/step\n",
      "Epoch 4/10\n",
      "1/1 - 0s - loss: 0.1657 - 1ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "1/1 - 0s - loss: 0.1657 - 1ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "1/1 - 0s - loss: 0.1657 - 987us/epoch - 987us/step\n",
      "Epoch 7/10\n",
      "1/1 - 0s - loss: 0.1657 - 977us/epoch - 977us/step\n",
      "Epoch 8/10\n",
      "1/1 - 0s - loss: 0.1657 - 968us/epoch - 968us/step\n",
      "Epoch 9/10\n",
      "1/1 - 0s - loss: 0.1656 - 984us/epoch - 984us/step\n",
      "Epoch 10/10\n",
      "1/1 - 0s - loss: 0.1656 - 988us/epoch - 988us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x155154100070>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note, if the loss is still going down, you can just keep running this cell to \n",
    "#  continue training\n",
    "\n",
    "red_lrate = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2,min_lr=0.0001, patience=10,verbose=1)\n",
    "\n",
    "my_attn_model.fit( [Xtrain,Postrain],Ytrain,batch_size=2,shuffle=True,epochs=10,verbose=2)\n",
    "my_attn_model.fit( [Xtrain,Postrain],Ytrain,batch_size=2,shuffle=True,epochs=5000,verbose=0,callbacks=[red_lrate])\n",
    "my_attn_model.fit( [Xtrain,Postrain],Ytrain,batch_size=2,shuffle=True,epochs=10,verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4LKedL6tA2VX",
    "outputId": "850f6552-d9a9-45e8-af73-5b7e65fa1ea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 106ms/step\n",
      "\n",
      " This is the output predictions use Head size 20\n",
      "          <ST>    the    man   chkn  ordrd  woman   beef\n",
      "0 <ST>   0.596  0.037  0.988  0.202  0.193  0.988  0.077\n",
      "1 man    0.062  0.579  0.575  0.243  0.996  0.184  0.802\n",
      "2 ordrd  0.092  0.991  0.122  0.612  0.319  0.050  0.403\n",
      "3 the    0.225  0.705  0.016  0.996  0.008  0.018  0.837\n",
      "4 chkn   0.994  0.103  0.365  0.679  0.104  0.634  0.333\n",
      "\n",
      " This is the output predictions use Head size 20\n",
      "          <ST>    the    man   chkn  ordrd  woman   beef\n",
      "0 <ST>   0.596  0.037  0.988  0.202  0.193  0.988  0.077\n",
      "1 woman  0.037  0.578  0.189  0.061  0.999  0.051  0.940\n",
      "2 ordrd  0.129  0.994  0.049  0.626  0.153  0.034  0.511\n",
      "3 the    0.028  0.686  0.163  0.727  0.786  0.034  0.992\n",
      "4 beef   0.994  0.107  0.369  0.679  0.099  0.633  0.340\n"
     ]
    }
   ],
   "source": [
    "Vout_pred =my_attn_model.predict([Xtrain,Postrain])\n",
    "\n",
    "import pandas as pd\n",
    "for bi in range(B):\n",
    "  Outputdf = pd.DataFrame(data=Vout_pred[bi], index=rownamesxb[bi], columns=colnames)\n",
    "  print('\\n This is the output predictions use Head size',H)  #<<<<------------check these out, are they good?\n",
    "  print(Outputdf.round(3).head(T))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9cu3dFtUwmZ"
   },
   "source": [
    "## Now we want to examine the Attetion Wt matrix see how those matrices affect 'the' predictions for each input sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rO5aN9zUeJ5",
    "outputId": "02b0e927-9524-47f0-a7a0-5b92227db2eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 93ms/step\n",
      "There are  14 layers with output activations\n"
     ]
    }
   ],
   "source": [
    "#get layer outputs (like we did with MNIST), to use below\n",
    "layer_outputs     = [layer.output for layer in my_attn_model.layers[:]]\n",
    "my_model_actvtns  = tf.keras.models.Model(inputs=my_attn_model.input, outputs=layer_outputs)\n",
    "my_actvtns_output = my_model_actvtns.predict([Xtrain,Postrain]) \n",
    "#my_actvtns_output = my_model_actvtns.predict(Xtrain) \n",
    "\n",
    "print('There are ',str(len(my_actvtns_output))+ ' layers with output activations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ttd8VsUNDN-F",
    "outputId": "117c279e-b88b-4230-9d2b-7027c797dfa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer num 0 Xseqids\n",
      "layer num 1 PosInput\n",
      "layer num 2 XEmbed\n",
      "layer num 3 PosEmbed\n",
      "layer num 4 XwithPos\n",
      "layer num 5 Qmat\n",
      "layer num 6 Kmat\n",
      "layer num 7 dot\n",
      "layer num 8 lambda\n",
      "layer num 9 AttnWts\n",
      "layer num 10 Vmat\n",
      "layer num 11 Voutput\n",
      "layer num 12 skipadd\n",
      "layer num 13 activation\n"
     ]
    }
   ],
   "source": [
    "#Print layer names and count layers\n",
    "layercnt=0\n",
    "for layer in my_attn_model.layers[:]:\n",
    "    print('layer num',layercnt,layer.name)\n",
    "    layercnt+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BVh9pe5dWcq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlSG0rNiD6jb",
    "outputId": "a6f2b21f-b9f8-4f4e-b0e4-7947a6a5625a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for i-th input: 0   These are the output at layer  AttnWts\n",
      "         0 <ST>  1 man  2 ordrd  3 the  4 chkn\n",
      "0 <ST>    1.000  0.000    0.000  0.000   0.000\n",
      "1 man     0.093  0.907    0.000  0.000   0.000\n",
      "2 ordrd   0.064  0.561    0.375  0.000   0.000\n",
      "3 the     0.003  0.213    0.759  0.025   0.000\n",
      "4 chkn    0.258  0.003    0.160  0.523   0.056\n",
      "The head size H was:  20\n",
      "for i-th input: 1   These are the output at layer  AttnWts\n",
      "         0 <ST>  1 woman  2 ordrd  3 the  4 beef\n",
      "0 <ST>    1.000    0.000    0.000  0.000   0.000\n",
      "1 woman   0.079    0.921    0.000  0.000   0.000\n",
      "2 ordrd   0.091    0.374    0.535  0.000   0.000\n",
      "3 the     0.001    0.769    0.223  0.007   0.000\n",
      "4 beef    0.252    0.001    0.162  0.526   0.059\n",
      "The head size H was:  20\n"
     ]
    }
   ],
   "source": [
    "#now get Attn Wts   \n",
    "\n",
    "#  <<<<<<< ------------ Can you inteprety how Attn Wts are picking out predictions  \n",
    "layernum2get=9\n",
    "AttnW_output = my_actvtns_output[layernum2get]\n",
    "\n",
    "for bi in range(B):\n",
    "\n",
    "  AttnW_outputdf = pd.DataFrame(data=AttnW_output[bi], index=rownamesxb[bi], columns=rownamesxb[bi])\n",
    "  print('for i-th input:',bi,'  These are the output at layer ',my_attn_model.layers[layernum2get].name)\n",
    "  print(AttnW_outputdf.round(3).head(T))\n",
    "  print('The head size H was: ',H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "we59lG9-FhtT"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
